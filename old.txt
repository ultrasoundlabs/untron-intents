This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
base58bruteforce/
  .cargo/
    config.toml
  src/
    main.rs
  Cargo.toml
.gitignore
blockchain.py
config.py
database.py
endpoints.py
main.py
old_transfers.py
polling.py
utils.py

================================================================
Files
================================================================

================
File: base58bruteforce/.cargo/config.toml
================
[build]
rustflags = "-Ctarget_cpu=native"

================
File: base58bruteforce/src/main.rs
================
use rayon::prelude::*;
use sha2::{Sha256, Digest};
use std::env;

// The Bitcoin Base58 alphabet (note that 0, O, I, and l are omitted)
const BASE58_ALPHABET: &[u8] =
    b"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz";

// Invalid marker for Base58 lookup table
const INVALID: u8 = 255;

// Precomputed lookup table for Base58 decoding
static BASE58_LOOKUP: [u8; 128] = {
    let mut table = [INVALID; 128];
    let mut i = 0;
    while i < BASE58_ALPHABET.len() {
        table[BASE58_ALPHABET[i] as usize] = i as u8;
        i += 1;
    }
    table
};

/// Decodes a Base58 string into bytes and validates if it's a valid address.
/// Returns None if any character is invalid or if the decoded bytes don't form a valid address.
#[inline]
fn base58_decode_and_validate(s: &str) -> Option<Vec<u8>> {
    // Count leading '1's
    let n_zeros = s.bytes().take_while(|&b| b == b'1').count();

    // Use a larger buffer for intermediate calculations (34 bytes should be enough)
    let mut num = [0u8; 34];
    let mut num_len = 1;

    for byte in s.bytes() {
        // Fast lookup using precomputed table
        let digit = if (byte as usize) < BASE58_LOOKUP.len() {
            BASE58_LOOKUP[byte as usize]
        } else {
            return None;
        };

        if digit == INVALID {
            return None;
        }

        // Optimized big number arithmetic with fixed buffer
        let mut carry = digit as u32;
        for i in (0..num_len).rev() {
            let v = (num[i] as u32) * 58 + carry;
            num[i] = (v & 0xff) as u8;
            carry = v >> 8;
        }

        while carry > 0 && num_len < num.len() {
            num.rotate_right(1);
            num[0] = carry as u8;
            num_len += 1;
            carry = 0;
        }
    }

    // Trim leading zeros from the calculated number
    let mut start_idx = 0;
    while start_idx < num_len && num[start_idx] == 0 {
        start_idx += 1;
    }

    // Check if we have exactly 25 bytes after trimming zeros
    if num_len - start_idx != 25 {
        return None;
    }

    // Check for valid address prefix
    if num[start_idx] != 0x41 {
        return None;
    }

    // Prepare final result with leading zeros
    let mut result = vec![0u8; n_zeros];
    result.extend_from_slice(&num[start_idx..num_len]);

    // Validate checksum
    if Sha256::digest(&Sha256::digest(&result[..21]))[..4] == result[21..] {
        Some(result)
    } else {
        None
    }
}

/// Generate a single candidate with the given bitmask
#[inline]
fn generate_candidate_with_mask(input: &str, mask: usize) -> String {
    let mut letter_idx = 0;
    input.chars()
        .map(|c| {
            if c.is_alphabetic() {
                let res = if (mask >> letter_idx) & 1 == 1 {
                    c.to_ascii_uppercase()
                } else {
                    c.to_ascii_lowercase()
                };
                letter_idx += 1;
                res
            } else {
                c
            }
        })
        .collect()
}

fn main() {
    let args: Vec<String> = env::args().collect();
    if args.len() != 2 {
        eprintln!("Usage: {} <ambiguous-base58-string>", args[0]);
        std::process::exit(1);
    }

    let input = &args[1];

    // Count letters for mask generation
    let num_letters = input.chars().filter(|c| c.is_alphabetic()).count();
    let total = 1 << num_letters;

    // Lazily generate and process candidates in parallel
    if let Some((cand, _)) = (0..total)
        .into_par_iter()
        .find_map_any(|mask| {
            let candidate = generate_candidate_with_mask(input, mask);
            if let Some(decoded) = base58_decode_and_validate(&candidate) {
                Some((candidate, decoded))
            } else {
                None
            }
        })
    {
        println!("{}", cand);
    }
}

================
File: base58bruteforce/Cargo.toml
================
[package]
name = "base58bruteforce"
version = "0.1.0"
edition = "2021"

[dependencies]
rayon = "1.10.0"
sha2 = { version = "0.10.8", features = ["asm"] }

================
File: .gitignore
================
binary
backups
receivers.db
logs

================
File: blockchain.py
================
import logging
from typing import Dict, Optional, Tuple
from web3 import AsyncWeb3, AsyncHTTPProvider
from web3.contract import AsyncContract

from .config import CONFIG, ABIS

logger = logging.getLogger(__name__)

class BlockchainClient:
    def __init__(self):
        logger.info("Initializing blockchain client")
        self.w3: AsyncWeb3 = AsyncWeb3(AsyncHTTPProvider(CONFIG["rpc_url"]))
        logger.info(f"Connected to RPC endpoint: {CONFIG['rpc_url']}")
        
        self.account = self.w3.eth.account.from_key(CONFIG["private_key"])
        logger.info(f"Initialized account: {self.account.address}")
        
        # Contract instances
        self.factory_contract: Optional[AsyncContract] = None
        self.usdt_contract: Optional[AsyncContract] = None
        self.usdc_contract: Optional[AsyncContract] = None

    async def setup_contracts(self) -> None:
        """Initialize contract instances."""
        logger.info("=== Setting up contract instances ===")
        
        # Set up factory contract
        logger.info(f"Setting up factory contract at {CONFIG['factory_address']}")
        self.factory_contract = self.w3.eth.contract(
            address=CONFIG["factory_address"],
            abi=ABIS["factory"]
        )
        
        # Set up token contracts
        logger.info("Fetching token addresses from factory contract")
        usdt_address = await self.factory_contract.functions.usdt().call()
        self.usdt_contract = self.w3.eth.contract(
            address=usdt_address,
            abi=ABIS["erc20"]
        )
        logger.info(f"Initialized USDT contract at {usdt_address}")
        
        usdc_address = await self.factory_contract.functions.usdc().call()
        self.usdc_contract = self.w3.eth.contract(
            address=usdc_address,
            abi=ABIS["erc20"]
        )
        logger.info(f"Initialized USDC contract at {usdc_address}")
        logger.info("=== Contract setup complete ===")

    async def generate_receiver_address(self, tron_bytes: bytes) -> str:
        """Generate receiver address from Tron address bytes."""
        if not self.factory_contract:
            raise RuntimeError("Factory contract not initialized")
        
        logger.info(f"Generating receiver address for Tron bytes: {tron_bytes.hex()}")
        address = await self.factory_contract.functions.generateReceiverAddress(tron_bytes).call()
        logger.info(f"Generated receiver address: {address}")
        return address

    async def _build_and_send_tx(self, func, gas_limit: int = 500000) -> Dict:
        """Helper to build and send a transaction."""
        nonce = await self.w3.eth.get_transaction_count(self.account.address)
        gas_price = await self.w3.eth.gas_price
        chain_id = await self.w3.eth.chain_id
        
        logger.info(f"Building transaction - Nonce: {nonce}, Gas Price: {gas_price}, Chain ID: {chain_id}")
        
        tx = await func.build_transaction({
            "chainId": chain_id,
            "gas": gas_limit,
            "gasPrice": gas_price,
            "nonce": nonce,
        })
        
        logger.info(f"Signing transaction: {tx}")
        signed_tx = self.account.sign_transaction(tx)
        
        logger.info("Sending transaction")
        tx_hash = await self.w3.eth.send_raw_transaction(signed_tx.raw_transaction)
        logger.info(f"Transaction sent, hash: {tx_hash.hex()}")
        
        logger.info("Waiting for transaction receipt")
        receipt = await self.w3.eth.wait_for_transaction_receipt(tx_hash)
        logger.info(f"Transaction confirmed in block {receipt['blockNumber']}")
        
        return receipt

    async def deploy_receiver(self, tron_address: bytes) -> Dict:
        """Deploy a new receiver contract."""
        if not self.factory_contract:
            raise RuntimeError("Factory contract not initialized")
            
        try:
            logger.info(f"=== Deploying receiver for Tron address: {tron_address.hex()} ===")
            receipt = await self._build_and_send_tx(
                self.factory_contract.functions.deploy(tron_address)
            )
            logger.info(f"Receiver deployment successful, gas used: {receipt['gasUsed']}")
            return receipt
        except Exception as e:
            logger.exception(f"Error deploying receiver for {tron_address.hex()}: {e}")
            raise

    async def call_intron(self, tron_address: bytes) -> Dict:
        """Call the intron function."""
        if not self.factory_contract:
            raise RuntimeError("Factory contract not initialized")
            
        try:
            logger.info(f"=== Calling intron for Tron address: {tron_address.hex()} ===")
            receipt = await self._build_and_send_tx(
                self.factory_contract.functions.intron(tron_address)
            )
            logger.info(f"Intron call successful, gas used: {receipt['gasUsed']}")
            return receipt
        except Exception as e:
            logger.exception(f"Error calling intron for {tron_address.hex()}: {e}")
            raise

    def get_token_contracts(self) -> Tuple[AsyncContract, AsyncContract]:
        """Get USDT and USDC contract instances."""
        if not all([self.usdt_contract, self.usdc_contract]):
            raise RuntimeError("Token contracts not initialized")
        return self.usdt_contract, self.usdc_contract

# Global blockchain client instance
client = BlockchainClient()

================
File: config.py
================
import json
import logging.config
import os
import datetime
from typing import Dict, Any
from pathlib import Path

# Get the project root directory
PROJECT_ROOT = Path(__file__).parent.parent.absolute()

# Constants
DB_FILENAME = str(PROJECT_ROOT / "receivers.db")

# Load configuration from JSON
def load_config() -> Dict[str, Any]:
    with open(PROJECT_ROOT / "config.json") as f:
        return json.load(f)

# Load ABIs
def load_abis() -> Dict[str, Any]:
    FACTORY_ABI = json.load(open(PROJECT_ROOT / "out/ReceiverFactory.json"))["abi"]
    RECEIVER_ABI = json.load(open(PROJECT_ROOT / "out/UntronReceiver.json"))["abi"]
    ERC20_ABI = [
        {
            "anonymous": False,
            "inputs": [
                {"indexed": True, "name": "from", "type": "address"},
                {"indexed": True, "name": "to", "type": "address"},
                {"indexed": False, "name": "value", "type": "uint256"}
            ],
            "name": "Transfer",
            "type": "event"
        }
    ]
    return {
        "factory": FACTORY_ABI,
        "receiver": RECEIVER_ABI,
        "erc20": ERC20_ABI
    }

def setup_logging() -> None:
    os.makedirs(PROJECT_ROOT / "logs", exist_ok=True)
    
    LOGGING_CONFIG = {
        "version": 1,
        "formatters": {
            "default": {
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            },
        },
        "handlers": {
            "file": {
                "class": "logging.handlers.RotatingFileHandler",
                "filename": str(PROJECT_ROOT / "logs" / "relayer.log"),
                "maxBytes": 10*1024*1024,  # 10MB
                "backupCount": 5,
                "formatter": "default",
            },
            "console": {
                "class": "logging.StreamHandler",
                "formatter": "default"
            }
        },
        "root": {
            "handlers": ["file", "console"],
            "level": "INFO",
        },
    }
    
    logging.config.dictConfig(LOGGING_CONFIG)

# Initialize configuration
CONFIG = load_config()
ABIS = load_abis()
setup_logging()

# Create logger for this module
logger = logging.getLogger(__name__)

================
File: database.py
================
import aiosqlite
import logging
from contextlib import asynccontextmanager
from typing import Optional, List, Dict, Any
from datetime import datetime

from .config import DB_FILENAME

logger = logging.getLogger(__name__)

@asynccontextmanager
async def get_db():
    """Context manager for database connections."""
    db = await aiosqlite.connect(DB_FILENAME)
    try:
        yield db
    finally:
        await db.close()

async def setup_database() -> None:
    """Initialize database tables."""
    logger.info(f"Setting up database: {DB_FILENAME}")
    async with get_db() as db:
        # Create receivers table
        logger.info("Creating receivers table if not exists")
        await db.execute("""
            CREATE TABLE IF NOT EXISTS receivers (
                tron_address TEXT PRIMARY KEY,
                receiver_address TEXT,
                resolved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Create case_fixes table
        logger.info("Creating case_fixes table if not exists")
        await db.execute("""
            CREATE TABLE IF NOT EXISTS case_fixes (
                original_address TEXT PRIMARY KEY,
                fixed_address TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        await db.commit()
        logger.info("Database setup complete")

async def get_case_fix(address: str) -> Optional[str]:
    """Get cached case fix for an address."""
    async with get_db() as db:
        async with db.execute(
            "SELECT fixed_address FROM case_fixes WHERE original_address = ?",
            (address,)
        ) as cursor:
            row = await cursor.fetchone()
            return row[0] if row else None

async def store_case_fix(original: str, fixed: str) -> None:
    """Store a case fix in the cache."""
    async with get_db() as db:
        await db.execute(
            "INSERT OR IGNORE INTO case_fixes (original_address, fixed_address) VALUES (?, ?)",
            (original, fixed)
        )
        await db.commit()
        logger.info(f"Cached case fix for {original}")

async def store_resolved_pair(tron_address: str, receiver_address: str) -> None:
    """Store a Tron->receiver address mapping."""
    logger.info(f"Storing resolved pair - Tron: {tron_address}, Receiver: {receiver_address}")
    async with get_db() as db:
        await db.execute(
            "INSERT OR IGNORE INTO receivers (tron_address, receiver_address) VALUES (?, ?)",
            (tron_address, receiver_address)
        )
        await db.commit()
        logger.info("Successfully stored resolved pair")

async def get_all_receivers() -> List[Dict[str, Any]]:
    """Get all stored Tron->receiver mappings."""
    async with get_db() as db:
        async with db.execute(
            "SELECT tron_address, receiver_address, resolved_at FROM receivers"
        ) as cursor:
            rows = await cursor.fetchall()
            return [
                {
                    "tron_address": row[0],
                    "receiver_address": row[1],
                    "resolved_at": row[2]
                }
                for row in rows
            ]

async def get_receiver_mapping() -> Dict[str, str]:
    """Get mapping of receiver_address -> tron_address."""
    async with get_db() as db:
        async with db.execute(
            "SELECT tron_address, receiver_address FROM receivers"
        ) as cursor:
            rows = await cursor.fetchall()
            return {row[1]: row[0] for row in rows}

================
File: endpoints.py
================
import logging
from aiohttp import web
from typing import Dict, Any

from .database import store_resolved_pair, get_all_receivers, get_case_fix, store_case_fix
from .blockchain import client
from .utils import run_case_fix_binary, validate_tron_address, decode_tron_address

logger = logging.getLogger(__name__)

async def resolve_handler(request: web.Request) -> web.Response:
    """Handle /resolve endpoint."""
    logger.info("=== Starting new resolve request ===")
    try:
        data = await request.json()
        logger.info(f"Received request data: {data}")
        
        domain = data.get("data")
        if not domain:
            logger.error("Missing data field in request")
            return web.json_response(
                {"message": "Missing data field"},
                status=400
            )
            
        try:
            domain = bytes.fromhex(domain.lstrip("0x"))
            logger.info(f"Successfully decoded domain bytes: {domain.hex()}")
        except Exception as e:
            logger.error(f"Failed to decode domain hex: {domain}, error: {e}")
            return web.json_response(
                {"message": f"Invalid domain format: {e}"},
                status=400
            )
            
        # Extract Tron address from domain data
        lowercased_tron_address = domain[1:domain[0]+1].decode().lower()
        logger.info(f"Extracted Tron address from domain: {lowercased_tron_address}")
        
        if not validate_tron_address(lowercased_tron_address):
            logger.error(f"Invalid Tron address format: {lowercased_tron_address}")
            return web.json_response(
                {"message": "Invalid Tron address format"},
                status=400
            )
            
        # Check cache for case fix
        fixed_tron_address = await get_case_fix(lowercased_tron_address)
        if fixed_tron_address:
            logger.info(f"Found cached case fix: {lowercased_tron_address} -> {fixed_tron_address}")
        else:
            logger.info(f"No cached case fix found for {lowercased_tron_address}, running binary...")
            # Run case fix binary if not in cache
            fixed_tron_address = await run_case_fix_binary(lowercased_tron_address)
            if fixed_tron_address:
                logger.info(f"Successfully fixed case: {lowercased_tron_address} -> {fixed_tron_address}")
                await store_case_fix(lowercased_tron_address, fixed_tron_address)
            else:
                logger.error(f"Failed to fix case for address: {lowercased_tron_address}")
                return web.json_response(
                    {"message": "Failed to process Tron address"},
                    status=500
                )
                
        # Generate receiver address
        tron_bytes = decode_tron_address(fixed_tron_address)
        if not tron_bytes:
            logger.error(f"Failed to decode fixed Tron address: {fixed_tron_address}")
            return web.json_response(
                {"message": "Invalid Tron address"},
                status=400
            )
            
        logger.info(f"Generating receiver address for Tron bytes: {tron_bytes.hex()}")
        receiver_address = await client.generate_receiver_address(tron_bytes)
        logger.info(f"Generated receiver address: {receiver_address}")
        
        # Store the mapping
        await store_resolved_pair(fixed_tron_address, receiver_address)
        logger.info(f"Stored mapping: {fixed_tron_address} -> {receiver_address}")
        
        # Construct response
        result = "0x" + (bytes([domain[0]]) + fixed_tron_address.encode() + domain[domain[0]+1:]).hex()
        logger.info(f"=== Resolve complete: {lowercased_tron_address} -> {result} ===")
        
        return web.json_response({"data": result})
        
    except Exception as e:
        logger.exception(f"Unexpected error in resolve_handler: {e}")
        return web.json_response(
            {"message": f"Internal server error: {str(e)}"},
            status=500
        )

async def list_receivers_handler(request: web.Request) -> web.Response:
    """Handle /receivers endpoint."""
    logger.info("=== Starting list_receivers request ===")
    try:
        receivers = await get_all_receivers()
        logger.info(f"Found {len(receivers)} receiver mappings")
        return web.json_response(receivers)
    except Exception as e:
        logger.exception(f"Error in list_receivers_handler: {e}")
        return web.json_response(
            {"message": f"Internal server error: {str(e)}"},
            status=500
        )

def setup_routes(app: web.Application) -> None:
    """Configure routes for the application."""
    logger.info("Setting up HTTP routes")
    app.router.add_post("/resolve", resolve_handler)
    app.router.add_get("/receivers", list_receivers_handler)
    logger.info("HTTP routes configured successfully")

================
File: main.py
================
import asyncio
import logging
import os
from aiohttp import web
from pathlib import Path

from .config import setup_logging, PROJECT_ROOT
from .database import setup_database
from .blockchain import client
from .endpoints import setup_routes
from .polling import poll_transfers

logger = logging.getLogger(__name__)

# Get the relayer directory path
RELAYER_DIR = Path(__file__).parent.absolute()

async def init_app() -> web.Application:
    """Initialize the web application."""
    logger.info("Initializing web application")
    app = web.Application()
    setup_routes(app)
    logger.info("Web application routes configured")
    return app

async def main() -> None:
    """Main application entry point."""
    logger.info("Starting application")
    
    # Change to base58bruteforce directory and build binary
    logger.info("Building base58bruteforce binary")
    original_dir = os.getcwd()
    os.chdir(RELAYER_DIR / "base58bruteforce")
    os.system("cargo build --release")
    os.chdir(original_dir)
    os.system(f"cp {RELAYER_DIR}/base58bruteforce/target/release/base58bruteforce {RELAYER_DIR}/binary")
    
    # Initialize database
    await setup_database()
    
    # Set up blockchain contracts
    await client.setup_contracts()
    
    # Initialize and start web application
    app = await init_app()
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 8454)
    await site.start()
    logger.info("Server started at http://0.0.0.0:8454")
    
    # Start transfer polling task
    logger.info("Starting transfer polling task")
    asyncio.create_task(poll_transfers())
    
    # Keep the application running
    while True:
        await asyncio.sleep(3600)

if __name__ == '__main__':
    try:
        logger.info("Starting application main loop")
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Server stopped by user")

================
File: old_transfers.py
================
import json
import asyncio
import os
from web3 import Web3
from base58 import b58encode_check

# Tron-related imports
from tronpy import Tron
from tronpy.providers import HTTPProvider
from tronpy.keys import PrivateKey

# ------------------------------------------------------------------------------
# Load config
# ------------------------------------------------------------------------------
config = json.load(open("config.json"))

# ------------------------------------------------------------------------------
# Initialize Tron client (or your Tron API provider of choice)
# ------------------------------------------------------------------------------
client = Tron(
    provider=HTTPProvider(
        "https://api.trongrid.io",
        api_key=config["trongrid_api_key"]
    )
)

private_key = PrivateKey(bytes.fromhex(config["tron_private_key"][2:]))
from_address = private_key.public_key.to_base58check_address()
print(f"Relayer Tron address: {from_address}")

# If you need to do a specialized USDT swap, get your DEX contract, e.g.:
sunswap_v2 = client.get_contract("TXF1xDbVGdxFGbovmmmXvBGu8ZiE3Lq4mR")  # Example

# ------------------------------------------------------------------------------
# Load UntronTransfers ABI
# ------------------------------------------------------------------------------
with open("../out/UntronTransfers.json") as f:
    untron_transfers_artifact = json.load(f)
untron_abi = untron_transfers_artifact["abi"]

# ------------------------------------------------------------------------------
# Helpers for Tron side
# ------------------------------------------------------------------------------
async def send_usdt(to_address: str, amount: int):
    """
    Example function that performs a Tron-based token transfer or swap.
    Here, we do a "swapTokensForExactTokens" on SunSwap as a demonstration.
    Adjust to your real logic for transferring USDT, USDC, or other tokens.
    """
    print(f"[Tron] Swapping into USDT => to: {to_address} amount: {amount}")
    try:
        txn = (
            sunswap_v2.functions.swapTokensForExactTokens(
                amount,
                999999999999999999999999,  # large max input
                [
                    "TPXxtMtQg95VX8JRCiQ5SXqSeHjuNaMsxi",  # e.g. TRX -> USDT route
                    "TR7NHqjeKQxGTCi8q8ZY4pL8otSzgjLj6t"
                ],
                to_address,
                9999999999  # large deadline
            )
            .with_owner(from_address)
            .fee_limit(2_000_000)
            .build()
            .sign(private_key)
        )
        receipt = txn.broadcast().wait()
        print(f"[Tron] Swap/transfer tx result: {receipt}")
        return True
    except Exception as e:
        print(f"[Tron] Error sending USDT: {e}")
        return False

def is_profitable(chain, order):
    """
    Check if an order is profitable based on configured fees for the token on the specific chain.
    Uses basis points (1/10000) for percentage calculations to avoid floats.
    Returns False if token is not in allowed list for the chain.
    """
    token = order["token"]
    if token not in chain["tokens"]:
        print(f"Token {token} not in allowed tokens list for chain {chain['name']}")
        return False

    token_config = chain["tokens"][token]
    input_amount = order["inputAmount"]
    output_amount = order["outputAmount"]
    
    # Calculate total fee (static + percentage)
    percentage_fee = (output_amount * token_config["percentage_fee_bps"]) // 10000
    total_fee = token_config["static_fee"] + percentage_fee
    
    # Order is profitable if input covers output plus fees
    is_profitable = input_amount >= (output_amount + total_fee)

    print("input_amount", input_amount)
    print("output_amount", output_amount)
    print("total_fee", total_fee)
    print("is_profitable", is_profitable)

    if not is_profitable:
        print(f"[{chain['name']}] Order not profitable - Input: {input_amount}, Output: {output_amount}, Fee: {total_fee}")
    
    return is_profitable

# ------------------------------------------------------------------------------
# Reclaim or claim on UntronTransfers (in the new contract, it's named 'claim')
# ------------------------------------------------------------------------------
def claim_order(web3, contract, account, order_id):
    """
    Once your Tron side is done, call contract.functions.claim(orderId).
    This finalizes the order and emits OrderFilled.
    """
    print(f"[EVM] Claiming order {order_id.hex()} ...")
    try:
        tx = contract.functions.claim(order_id).build_transaction({
            "from": account.address,
            "nonce": web3.eth.get_transaction_count(account.address),
            "gas": 3000000,
            "gasPrice": web3.eth.gas_price,
        })
        signed_tx = account.sign_transaction(tx)
        tx_hash = web3.eth.send_raw_transaction(signed_tx.raw_transaction)
        receipt = web3.eth.wait_for_transaction_receipt(tx_hash)
        print(f"[EVM] claim() success. Tx hash: {receipt['transactionHash'].hex()}")
    except Exception as e:
        print(f"[EVM] Failed to claim order: {e}")

# ------------------------------------------------------------------------------
# Process the OrderCreated event
# ------------------------------------------------------------------------------
async def process_order_created_event(web3, contract, account, event, chain):
    """
    Called when we see an OrderCreated log. 
    'event' includes orderId + order struct:
      order = (refundBeneficiary, token, inputAmount, to, outputAmount, deadline)
    """
    args = event["args"]
    order_id = args["orderId"]
    order = args["order"]

    print(f"New OrderCreated event. orderId = {order_id.hex()}")
    print("Order struct:", order)

    # Basic check (optional)
    if not is_profitable(chain, order):
        print("Order not considered profitable; skipping.")
        return

    # Convert `bytes20 to` to a Tron base58 address
    raw_addr = b"\x41" + order["to"]
    to_address = b58encode_check(raw_addr).decode()
    print(f"Tron recipient: {to_address}")

    # The amount to deliver on Tron side:
    amount = order["outputAmount"]

    # Example: do a Tron side transfer or swap
    success = await send_usdt(to_address, amount)
    if not success:
        print(f"Tron side transfer failed for order {order_id.hex()}")
        return

    # If Tron side is successful, call `claim(orderId)` on UntronTransfers
    claim_order(web3, contract, account, order_id)

# ------------------------------------------------------------------------------
# Utilities to store last processed block (so we can resume)
# ------------------------------------------------------------------------------
LAST_BLOCK_FILE_TEMPLATE = "backups/last_block_{}.txt"

def save_last_block(chain_name, block_number):
    os.makedirs(os.path.dirname(LAST_BLOCK_FILE_TEMPLATE.format(chain_name)), exist_ok=True)
    with open(LAST_BLOCK_FILE_TEMPLATE.format(chain_name), "w") as f:
        f.write(str(block_number))

def load_last_block(chain_name):
    file_path = LAST_BLOCK_FILE_TEMPLATE.format(chain_name)
    if os.path.exists(file_path):
        with open(file_path, "r") as f:
            return int(f.read().strip())
    return None

# ------------------------------------------------------------------------------
# Listen for new blocks, parse transactions for OrderCreated logs
# ------------------------------------------------------------------------------
async def listen_for_orders(chain):
    web3 = Web3(Web3.HTTPProvider(chain["rpc"]))
    contract = web3.eth.contract(address=chain["contract_address"], abi=untron_abi)

    # The local EVM account used to sign claims
    account = web3.eth.account.from_key(config["ethereum_private_key"])

    # Verify this account is the trusted relayer
    trusted_relayer = contract.functions.trustedRelayer().call()
    if trusted_relayer.lower() != account.address.lower():
        print(f"[{chain['name']}] Account {account.address} is not the trusted relayer {trusted_relayer}")
        exit(1)

    chain_name = chain["name"]
    print(f"[{chain_name}] Listening for OrderCreated events at {chain['contract_address']}")

    # Resume from last saved block or current
    last_block = load_last_block(chain_name) or web3.eth.block_number
    print(f"[{chain_name}] Starting from block {last_block}")

    # Get the event signature for OrderCreated
    order_created_event = contract.events.OrderCreated()
    event_signature = "0x" + Web3.keccak(text="OrderCreated(bytes32,(address,address,uint256,bytes20,uint256,uint256))").hex()

    while True:
        current_block = web3.eth.block_number
        # If new blocks are available, process them in chunks
        if current_block > last_block:
            chunk_size = 1000  # Adjust based on your RPC provider's limits
            from_block = last_block + 1

            while from_block <= current_block:
                to_block = min(from_block + chunk_size - 1, current_block)

                try:
                    logs = web3.eth.get_logs({
                        "fromBlock": from_block,
                        "toBlock": to_block,
                        "address": chain["contract_address"],
                        "topics": [event_signature]  # Filter by OrderCreated event signature
                    })

                    for log in logs:
                        try:
                            event = order_created_event.process_log(log)
                            print(f"[{chain_name}] Found OrderCreated event in block {log['blockNumber']}")
                            await process_order_created_event(web3, contract, account, event, chain)
                        except Exception as e:
                            print(f"[{chain_name}] Error processing log: {e}")
                            continue

                except Exception as e:
                    print(f"[{chain_name}] Error fetching logs for blocks {from_block}-{to_block}: {e}")
                    # If the chunk size is too large, we could implement retry logic with smaller chunks
                    # For now, we'll just continue to the next chunk
                    pass

                from_block = to_block + 1

            last_block = current_block
            save_last_block(chain_name, last_block)

        await asyncio.sleep(2)  # Poll interval in seconds

# ------------------------------------------------------------------------------
# Main entry (launches async tasks for each chain in config)
# ------------------------------------------------------------------------------
async def main():
    print("Initializing UntronTransfers relayer...")
    tasks = []
    for chain in config["chains"]:
        print(f"Launching listener for chain: {chain['name']}")
        tasks.append(asyncio.create_task(listen_for_orders(chain)))

    print("All chain listeners launched.")
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    while True:
        try:
            asyncio.run(main())
        except Exception as e:
            print(f"Top-level error: {e}")

================
File: polling.py
================
import asyncio
import logging
from typing import List, Dict, Optional
from web3.contract import AsyncContract
import time

from .database import get_receiver_mapping
from .blockchain import client
from .utils import address_to_topic, save_last_block, load_last_block, decode_tron_address

logger = logging.getLogger(__name__)

# Status update interval (in seconds)
STATUS_UPDATE_INTERVAL = 60
last_status_update = 0

async def process_token_transfers(
    token_contract: AsyncContract,
    receiver_addresses: List[str],
    from_block: int,
    to_block: int
) -> None:
    """Process transfer logs for a specific token contract."""
    token_name = "USDT" if token_contract.address == client.usdt_contract.address else "USDC"
    
    try:
        # Get transfer logs
        logs = await client.w3.eth.get_logs({
            "fromBlock": from_block,
            "toBlock": to_block,
            "address": token_contract.address,
            "topics": [
                "0x" + client.w3.keccak(text="Transfer(address,address,uint256)").hex(),
                None,
                [address_to_topic(x) for x in receiver_addresses]
            ]
        })
        
        if logs:
            logger.info(f"Found {len(logs)} {token_name} transfer events in blocks {from_block}-{to_block}")
        
        for log in logs:
            try:
                # Process the Transfer event
                event = token_contract.events.Transfer().process_log(log)
                to_address = event.args.to
                value = event.args.value
                logger.info(f"Processing {token_name} transfer: {value} tokens to {to_address}")
                
                # Get corresponding Tron address
                mapping = await get_receiver_mapping()
                tron_address = mapping.get(to_address)
                if not tron_address:
                    logger.error(f"No Tron address mapping found for receiver {to_address}")
                    continue
                
                # Decode Tron address
                tron_bytes = decode_tron_address(tron_address)
                if not tron_bytes:
                    logger.error(f"Failed to decode Tron address: {tron_address}")
                    continue
                
                # Call intron()
                logger.info(f"Calling intron() for transfer to {to_address}")
                receipt = await client.call_intron(tron_bytes)
                logger.info(f"Intron call successful for {token_name} transfer, tx: {receipt['transactionHash'].hex()}")
                
            except Exception as e:
                logger.exception(f"Error processing {token_name} transfer event: {e}")
                continue
                
    except Exception as e:
        logger.exception(f"Error fetching {token_name} logs for blocks {from_block}-{to_block}: {e}")
        raise

async def poll_transfers() -> None:
    """
    Background task that polls for Transfer events from USDT and USDC contracts
    to any receiver address stored in the database.
    """
    global last_status_update
    logger.info("=== Starting transfer polling background task ===")
    
    # Get Transfer event signature
    transfer_event_signature = client.w3.keccak(text="Transfer(address,address,uint256)").hex()
    
    chunk_size = 1000  # Initial chunk size
    min_chunk_size = 100  # Minimum chunk size when reducing due to errors
    
    while True:
        try:
            current_time = time.time()
            
            # Get current receiver mappings
            mapping = await get_receiver_mapping()
            receiver_addresses = list(mapping.keys())
            
            # Emit periodic status update
            if current_time - last_status_update >= STATUS_UPDATE_INTERVAL:
                logger.info(f"Status: Monitoring {len(receiver_addresses)} receiver addresses for USDT/USDC transfers")
                last_status_update = current_time
            
            if receiver_addresses:
                for token_contract in [client.usdt_contract, client.usdc_contract]:
                    token_name = "USDT" if token_contract.address == client.usdt_contract.address else "USDC"
                    
                    # Load last processed block
                    last_block = await load_last_block(f"{token_name}_transfers")
                    if not last_block:
                        last_block = await client.w3.eth.block_number
                    
                    current_block = await client.w3.eth.block_number
                    
                    if current_block > last_block:
                        blocks_behind = current_block - last_block
                        if blocks_behind > 100:  # Only log if significantly behind
                            logger.info(f"Processing {blocks_behind} new blocks for {token_name}")
                        
                        from_block = last_block + 1
                        while from_block <= current_block:
                            to_block = min(from_block + chunk_size - 1, current_block)
                            
                            try:
                                await process_token_transfers(
                                    token_contract,
                                    receiver_addresses,
                                    from_block,
                                    to_block
                                )
                                
                                # Update last processed block
                                await save_last_block(f"{token_name}_transfers", to_block)
                                from_block = to_block + 1
                                
                                # Reset chunk size if successful
                                chunk_size = 1000
                                
                            except Exception as e:
                                logger.error(f"Error processing chunk {from_block}-{to_block}: {e}")
                                # Reduce chunk size and retry
                                chunk_size = max(chunk_size // 2, min_chunk_size)
                                # Don't advance from_block so we retry this chunk
                                await asyncio.sleep(1)  # Brief pause before retry
            
        except Exception as e:
            logger.exception(f"Error in polling loop: {e}")
            
        # Wait before next polling iteration
        await asyncio.sleep(2)

================
File: utils.py
================
import asyncio
import logging
import os
from typing import Optional
import base58
from pathlib import Path

from .config import PROJECT_ROOT

logger = logging.getLogger(__name__)

# Get the relayer directory path
RELAYER_DIR = Path(__file__).parent.absolute()

async def run_case_fix_binary(address: str) -> str:
    """Run the base58bruteforce binary asynchronously."""
    try:
        process = await asyncio.create_subprocess_exec(
            str(RELAYER_DIR / "binary"), address,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await process.communicate()
        if stdout:
            return stdout.decode().strip()
        if stderr:
            logger.error(f"Error from case fix binary: {stderr.decode()}")
        return ""
    except Exception as e:
        logger.exception(f"Error running case fix binary: {e}")
        return ""

def address_to_topic(address: str) -> str:
    """Convert an Ethereum address to a 32-byte topic."""
    if address.startswith("0x"):
        address = address[2:]
    return "0x" + address.rjust(64, "0")

async def save_last_block(chain_name: str, block_number: int) -> None:
    """Save the last processed block number for a chain."""
    os.makedirs(PROJECT_ROOT / "backups", exist_ok=True)
    with open(PROJECT_ROOT / "backups" / f"last_block_{chain_name}.txt", "w") as f:
        f.write(str(block_number))

async def load_last_block(chain_name: str) -> int:
    """Load the last processed block number for a chain."""
    try:
        with open(PROJECT_ROOT / "backups" / f"last_block_{chain_name}.txt", "r") as f:
            return int(f.read().strip())
    except FileNotFoundError:
        return 0

def decode_tron_address(tron_address: str) -> Optional[bytes]:
    """Decode a Tron address to bytes, returning None if invalid."""
    try:
        return base58.b58decode_check(tron_address)[1:]
    except Exception as e:
        logger.error(f"Error decoding Tron address {tron_address}: {e}")
        return None

def validate_tron_address(tron_address: str) -> bool:
    """Validate a Tron address format."""
    if not isinstance(tron_address, str):
        return False
    if len(tron_address) not in range(25, 35):
        return False
    try:
        decode_tron_address(tron_address)
        return True
    except Exception:
        return False



================================================================
End of Codebase
================================================================
